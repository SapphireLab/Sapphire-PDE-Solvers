# VW-PINNs: A Volume Weighting Method for PDE Residuals in Physics-Informed Neural Networks

## Abstract
> Physics-informed neural networks (PINNs) have shown remarkable prospects inthe solving the forward and inverse problems involving partial differential equations(PDEs).
> The method embeds PDEs into the neural network by calculating PDE loss ata series of collocation points, providing advantages such as meshfree and moreconvenient adaptive sampling.
> However, when solving PDEs using nonuniformcollocation points, PINNs still face challenge regarding inefficient convergence ofPDE residuals or even failure.
> In this work, we first analyze the ill-conditioning of thePDE loss in PINNs under nonuniform collocation points.
> To address the issue, wedefine volume-weighted residual and propose volume-weighted physics-informedneural networks (VW-PINNs).
> Through weighting the PDE residuals by the volumethat the collocation points occupy within the computational domain, we embedexplicitly the spatial distribution characteristics of collocation points in the residualevaluation.
> The fast and sufficient convergence of the PDE residuals for the problemsinvolving nonuniform collocation points is guaranteed.
> Considering the meshfreecharacteristics of VW-PINNs, we also develop a volume approximation algorithmbased on kernel density estimation to calculate the volume of the collocation points.
> We verify the universality of VW-PINNs by solving the forward problems involvingflow over a circular cylinder and flow over the NACA0012 airfoil under differentinflow conditions, where conventional PINNs fail;
> By solving the Burgers’ equation,we verify that VW-PINNs can enhance the efficiency of existing the adaptivesampling method in solving the forward problem by 3 times, and can reduce therelative error of conventional PINNs in solving the inverse problem by more than oneorder of magnitude.

## 1.Introduction
> In recent years, physics-informed neural networks (PINNs) [1] have became aresearch hotspot for solving the forward and inverse problems involving partial differential equations (PDEs).
> The central idea of PINNs is integrating the governingequations of physical systems into the loss function of neural networks, ensuring thatthe neural networks minimize PDE residuals while approaching the definiteconditions or observed data.
> In fact, the concept of PINNs can be traced back to the20th century when Dissanayake et al.
> [2] pioneered the use of neural networks tosolve partial differential equations.
> Compared with mesh-based numerical methodssuch as finite element (FEM), finite difference (FDM), and finite volume (FVM), theadvantage of PINNs lies in their ability to directly calculate spatiotemporalderivatives through automatic differentiation (AD) [3], enabling a meshfree approachthat avoids mesh generation, which also indicates that PINNs can alleviate the curseof dimensionality to a certain extent [4-5].
> Moreover, PINNs can convenientlyintegrate various observed data for solving the inverse problems, such as inferringvelocity and pressure fields based on concentration field and the Navier-Stokesequations [6].
> In contrast, numerical methods demand extremely high cost to achievethe same process.
> PINNs have been demonstrated for various physics phenomena,including fluid mechanics [6-9], heat transfer [10-11], fluid-structure interaction [12-13], electromagnetic propagation [14], and quantum chemistry [15].

> Although PINNs have achieved gratifying performance, it still faces challengesin terms of accuracy, computational efficiency and training robustness, especially forcomplex problems [16].
> Therefore, over the past few years, numerous researchershave enhanced PINNs in various aspects.
> For example, adaptive activation functionshave been designed to improve the convergence of the algorithm [17], and hp-VPINNs based on the weak form of PDE have been proposed to perform integraltransformation on the PDE residuals through Legendre basis function [18].
> gPINNshave been developed to embed gradient information of PDE residuals into the lossfunction [19].
> For complex spatiotemporal problems, the parallel frameworks [20-21]have been built based on spatiotemporal domain decomposition to accelerate thetraining of PINNs and improve the accuracy.
> In order to enhance multi-scalerecognition capabilities, Fourier feature network [22] that scales spatio-temporalcoordinates in sinusoidal space and MscaleDNN [23] that constructs multi-scale inputa priori were developed.
> Furthermore, the gradient pathologies in PINNs have beenaddressed through adaptive weight [24-26] and hard constraints [27-29].

> In PINNs, PDE loss is evaluated at a set of scattered collocation points.
> Theeffect of collocation points on PINNs is similar to the effect of mesh points on FEM.
> Thus, the location and distribution of these collocation points should be highlyimportant to the performance of PINNs [30].
> When the solution of the PDE is asimple continuous smooth function, uniform sampling of the collocation points isusually appropriate.
> However, for many practical problems, such as flow over anobject and shock wave capture in fluid mechanics.
> The gradients of the solution at theobject and shock wave are large, while they are small in other computational regions.
> Thus, refining the collocation points near the object and shock wave to better capturethe details of flow while maintaining low-density sampling in other regions canconsider both computational cost and accuracy, which is already the consensus forsolving such problems by numerical methods.
> At present, although some researcheson PINNs have paid attention to the importance of nonuniform sampling [30-39],these studies still calculate the mean squared error of PDE residuals at all collocationpoints as PDE loss.
> This loss evaluation method overlooks the inconsistentconvergence of PDE residuals at different locations within the computational domaincaused by differences in sampling density.
> Specifically, in the PDE loss of PINNs, theproportion of PDE residuals at all collocation points is equal.
> The reduction of localPDE residuals within high-density regions brings greater benefits of loss reductioncompared with those within low-density regions when solving problems usingnonuniform collocation points.
> The network naturally focuses on reducing the PDEresiduals in these high-density sampling regions.
> Conversely, the residuals in low-density sampling regions have difficulty converging.
> Consequently, it diminishes theefficiency of PINNs and may even lead to failing, especially when there is asignificant difference in sampling density.
> We refer to this issue as the ill-conditioningof PDE loss function.

> To address the issue, this work defines volume-weighted residual and proposesvolume-weighted physics-informed neural networks (VW-PINNs).
> The PDE lossfunction is re-evaluated by weighting the PDE residuals based on the volumeoccupied by the collocation points within the computational domain.
> To calculate thevolume of collocation points in meshfree scenarios such as VW-PINNs, we alsodevelop an efficient volume approximation algorithm based on kernel densityestimation [40-41].
> By solving four forward problems and one inverse problem, weverify the advantages of the proposed method in terms of universality, convergence,and accuracy.

> The remainder of the paper is organized as follows.
> In Section 2, we provide a detailed introduction to PINNs and analyze the ill-conditioning of the PDE lossfunction in PINNs.
> Then, we propose VW-PINNs.
> In Section 3, We carry out a seriesof numerical experiments to verify the effectiveness of VW-PINNs.
> Concludingremarks and direction for future research are then presented in Section 4.

## 2.Methodology

### 2.1.PINNs

### 2.2.The ill-conditioning of PDE loss function in PINNs
> At present, most researches on PINNs generally compute the PDE loss accordingto formula (5), which means that the proportion of PDE residuals at all collocationpoints is equal in the loss.
> It is reasonable when employing uniform sampling forcollocation points, because the spatial distribution pattern of the collocation points isconsistent with their proportion in the loss.
> However, when employing nonuniformsampling, due to the sampling density disparities among collocation points at differentlocations in the computational domain, computing the loss according to formula (5)will lead to the domination of PDE residuals from high-density sampling regions inthe network training.
> The network parameters tend to sufficiently reduce the residualsin these regions, in contrast, other low-density regions receive little attention and the residuals converge difficultly.
> In other words, the decrease of conventional PDE losscan't guarantee the convergence of the total PDE residual in the entire computationaldomain.
> The reason is as follows: considering the high-density and low-densitysampling regions that occupy the same volume within the computational domain, it isevident that the former contains a greater number of collocation points than the latter.
> Thus, when the PDE residuals at all collocation points have an equal proportion in thePDE loss, the loss reduction benefits brought by reducing the local residuals in high-density regions are higher than those in low-density regions.
> Inevitably, the networksnaturally focus on local learning in these high-density regions.
> But as is well known,solving partial differential equations requires an adequate reduction in the residualsthroughout the entire computational domain.
> As according to the error propagationtheory, erroneous results in regions with high residuals can lead to inaccuracies in thesolution of other regions, even if the residuals in those regions are low [38].
> Therefore,for nonuniform sampling problems, conventional PDE loss exhibits obviouslimitations, it diminishes the solving efficiency of PINNs and may even lead tosolving failure, especially for complex nonlinear problems with significant differencesin sampling density.
> We take the flow over an object in fluid mechanics as an example to illustrate theabove issue, which is a typical nonuniform sampling problem.
> Because the gradient ofthe flow field is large near the object, a sufficiently dense set of collocation points isrequired to capture the flow details better.
> In contrast, the gradient in other regions issmaller, and sparse collocation points are adequate, which is the consensus for solvingsuch problems.
> However, current researches using PINNs to solve the forwardproblems involving flow over an object commonly relies on nearly uniformcollocation points within the channel [45-47] or within a specific local region [7,48].
> In these cases, PDEs are solved within small computational domains, which are onlyseveral times the size of the objects.
> Thus, it is acceptable to use dense collocationpoints in regions far from the object.
> However, for classical external flow problems atsubsonic or transonic speeds, the far-field boundary is infinite in principle and is atleast dozens of times larger than the object in practice.
> In such scenarios, uniformlysampling according to the resolution requirement near the object would result in anenormous number of collocation points, which leads to significantly highcomputational cost in solving problems.
> Therefore, nonuniform sampling is necessary.
> We use PINNs to solve the inviscid compressible flow over a circular cylinder and the collocation points obtained by nonuniform sampling.
> The governing equations for this problem are:
> $$
>
> $$
>
> In formula (7), 𝑢 denotes the 𝑥-component of the velocity field, 𝑣 the 𝑦-component.,T andp represent density, temperature and pressure respectively, satisfying therelationship:2/ ( )p T Ma  = (Ma is the Mach number).vc is the specific heat atconstant volume,1.4 = is the specific heat ratio, satisfying the relationship:2/ [( 1) ]vc Ma  = −.
> We set0.4Ma = , with[ , ]x y as the network inputs and[ , , , ]u v Tas the network outputs.

> The cylinder is placed at( , ) (0, 0)x y = with diameter1D = , while the far field ispositioned at the same center with a diameter of 40.
> The computational domain andcollocation points distribution are shown in Figure 2.
> The total number of collocationpoints is4800rN = .
> Meanwhile, we randomly select80bcN = points on the cylinderand apply the non-penetrating conditionV n = 0 .
> In the far field, we similarly choose80bcN =points randomly and specify the dimensionless variables as constants[ , , , ] [1,1, 0,1]u v T    =(Because the flow away from the object approaches theuniform freestream).
> We solve this problem using conventional PINNs with 5 hiddenlayers and 64 neurons per layer, the activation function chosen as tanh.
> For thenetwork training, we first run 3000 steps using the Adam optimizer, followed by anadditional 2000 steps using the L-BFGS optimizer (maximum number of inneriterations is 20).

> Figure 3 displays the convergence of loss functions obtained by PINNs.
> Evidently, both the boundary loss and the PDE loss are decreased by more than 5orders of magnitude, which usually means that the networks obtain acceptable results.
> However, the flow field given by PINNs are wrong, as shown in Figure 4.
> Thedistribution of PDE residuals shown in Figure 5 visually illustrates the reason for thesolving failure.
> While the residuals near the cylinder with high-density samplingconverge to the order of410− , the residuals in the far-field freestream region with low-density sampling only drop to the order of110− (Although the residuals in thefar-field freestream region are large, the averaged PDE loss remains low due to thesmall number of collocation points in this region).
> Which indicates that the total PDEresidual across the entire computational domain insufficiently converges.
> Theinformation from the far-field freestream fails to propagate to the vicinity of thecylinder, resulting in an inevitable failure of the solving.
> Our perspective receivesstrong validation from this result.

### 2.3.Volume-Weighted Physics-Informed Neural Networks (VW-PINNs)
> To address the issue described in Section 2.2, based on the volume occupied bycollocation points within the computational domain, we define the volume-weightedresidual.
> Taking partial differential equation (1) as an example, the volume-weightedresidual at collocation point( , )i i r rtx is:
> $$
>
> $$
>
> where( , )i i r rV tx is the volume occupied by( , )i i r rtx in the computational domain.
> Compared with the conventional calculation of only the residual of PDE at( , )i i r rtx ,we weight the residuals of PDE through the volume occupied by collocation points inthe computational domain, embedding explicitly the spatial distribution characteristicsof collocation points in the residual evaluation.
> Building on this, we propose volume-weighted physics-informed neural networks (VW-PINNs), establishing a novelmethod for evaluating the PDE loss function.
> Still taking the partial differentialequation (1) as an example, the PDE loss of VW-PINNs is:
>

> Compared with the PDE loss of conventional PINNs, VW-PINNs impose PDEconstraint through volume-weighted residual.
> The residual of PDE at each collocationpoint is weighted by the volume it occupies, and the weight satisfies a linear inverserelationship with the sampling density of collocation point.
> Therefore, our methodbalances the differences in residual convergence across regions with various samplingdensities in network optimization.
> In other words, the decrease of volume-weightedPDE loss can ensure effective convergence of the total PDE residual across the entirecomputational domain.
> The convergence level of the volume-weighted PDE lossdetermines the convergence status of the total residual of PDE.
> In addition, it isobvious that when uniform sampling is employed,( , ) /i i r r s rV t V N=x (sV is the volumeof the entire computational domain), formula (9) degenerates into formula (5).

> Since VW-PINNs is meshfree, it cannot compute volume relying on meshtopology as mesh-based numerical methods.
> Therefore, based on kernel densityestimation method [40-41], we develop a volume approximation algorithm suitablefor random sampling, allowing for the approximate calculation of volume atextremely low time cost.
> Kernel density estimation is a non-parametric statisticalmethod.
> Its fundamental concept involves placing a kernel function centered at eachsampling point, such as the Gaussian kernel function utilized in this study.
> Subsequently, all these kernel functions are superimposed to estimate density.
> Thismethod is widely applied in fields such as data analysis and machine learning.

> Before stating the volume approximation algorithm, it is important to note thatPINNs solve the PDE in the entire spatiotemporal dimensions directly.
> Thus, wechoose to incorporate the time coordinate into the volume calculation.
> Based on thecollocation points coordinate1{ , } rNi i r r it =x , the probability density function( , )p tx in thecomputational domain can be obtained:2 2 2 11 ( , ) exp( ) 2r i iN r r irt t p t N h=− + − = − x x x(10)where0h  is the bandwidth, which controls the radial scope of the kernel functions.
> Based on formula (10), the probability density( , )i i r rp tx at each collocation point canbe obtained.
> Calculate its reciprocal:1 ( , ) ( , )i i r r i i r ra t p t=x x(11)normalize( , )i i r ra tx :
>
> Finally, by multiplyingnorma with the total volumesV of the entire computationaldomain, the approximate value of the volume occupied by each collocation point inthe computational domain is obtained.
> A schematic illustration of VW-PINNs isshown in Figure 6.

## 3.Results
### 3.1.Inviscid compressible flow over a circular cylinder
### 3.2.Viscous incompressible flow over a circular cylinder
### 3.3.Viscous incompressible flow over the NACA0012 airfoil
### 3.4.Forward problem involving Burgers’ equation based on adaptive sampling
### 3.5.Inverse problem involving Burgers’ equation

## 4.Conclusions
> In this work, we first study the ill-conditioning of the PDE loss function forPINNs under nonuniform collocation points.
> Since the proportion of PDE residuals atall sampling points in the PDE loss is equal, when employing nonuniform sampling,the loss reduction benefits brought by reducing the local residuals in high-densityregions are higher than those in low-density regions.
> Inevitably, the network willnaturally focus on reducing residuals in high-density regions.
> In contrast, other low-density regions receive little attention and the residuals converge difficultly, resultingin the poor performance of PINNs.
> By solving the inviscid compressible flow over acircular cylinder, we analyze this problem specifically.

> To address this limitation of PINNs, we define the volume-weighted residual andpropose volume-weighted physics-informed neural networks (VW-PINNs).
> Throughweighting the PDE residuals by the volume that the collocation points occupy withinthe computational domain, we embed explicitly the spatial distribution characteristics of collocation points in the residual evaluation.
> Relying on the linear inverserelationship between the sampling density of collocation points and the volume theyoccupy, VW-PINNs balance the differences in residual convergence across regionswith various sampling densities in network optimization.
> The decrease of the volume-weighted PDE loss ensures efficient convergence of the total PDE residual across theentire computational domain.
> To calculate the volume of collocation points, we alsodevelop a volume approximation algorithm suitable for random sampling based on thekernel density estimation method.
> It only requires the coordinates of sampling pointsto calculate the approximate volume.
> By solving four forward problems and oneinverse problem, we verify the effectiveness of the proposed method:
> 1. VW-PINNs successfully solve the flow over a circular cylinder and the flowover the NACA0012 airfoil under different inflow conditions where conventionalPINNs fail, with the relative errors of wall pressure coefficient only about 1% andaccurate capture of the flow separation behind the cylinder.
> 2. By incorporating the volume weighting method into the existing adaptivesampling approach, we triple the solving efficiency of the adaptive sampling approach.
> 3. In solving the inverse PDE problem with nonuniform sampling, comparedwith PINNs, VW-PINNs reduce the relative error by an order of magnitude.

> However, our method still faces challenges in solving flows at high Reynoldsnumbers and flow over the airfoil under compressible condition, which is possiblydue to the lack of fitting ability of neural networks for flow with significant scaledifferences.
> These challenges may be solved in the future by using recent advances inthe field.