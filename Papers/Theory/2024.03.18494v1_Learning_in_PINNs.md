# Learning in PINNs: Phase Transition, Total Diffusion and Generalization

## Abstract
> We investigate the learning dynamics of fully-connected neural networks through the lens of gradient <u>*signal-to-noise ratio (SNR)*</u>, examining the behavior of first-order optimizers like Adam in non-convex objectives.
> By interpreting the drift/diffusion phases in the information bottleneck theory, focusing on gradient homogeneity, we identify a third phase termed “total diffusion”, characterized by equilibrium in the learning rates and homogeneous gradients.
> This phase is marked by an abrupt SNR increase, uniform residuals across the sample space and the most rapid training convergence.
> We propose a residual-based reweighting scheme to accelerate this diffusion in quadratic loss functions, enhancing generalization.
> We also explore the information compression phenomenon, pinpointing a significant saturation-induced compression of activations at the total diffusion phase, with deeper layers experiencing negligible information loss.
> Supported by experimental data on physics-informed neural networks (PINNs), which underscore the importance of gradient homogeneity due to their PDE-based sample inter-dependence, our findings suggest that recognizing phase transitions could refine ML optimization strategies for improved generalization.

我们通过梯度信号噪声比的视角研究了全连接神经网络中的学习动态, 研究了如 Adam 的一阶优化器在非凸目标中的行为.
通过解释信息瓶颈理论中的漂移/扩散阶段, 关注梯度同质性, 我们确定了一个称为 "完全扩散" 的第三阶段, 其特征是学习率平衡合均匀梯度.
这个阶段以 SNR 的突然增加, 样本空间中均匀的残差和最快的训练收敛为标志.
我们提出了一种基于残差的重加权方案, 以加速二次损失函数中的这种扩散, 提高泛化能力.
我们还探讨了信息压缩现象, 指出在完全扩散阶段, 由于激活饱和二导致的信息压缩, 而深层几乎没有信息损失.
通过 PINNs 的实验数据, 其表明了因为基于 PDE 的样本相互依赖性导致的梯度同质性的重要性, 我们的发现表明认识到相变可以改进优化策略以提高泛化能力.

## 1.Introduction
### 1.1.Phase Transitions in Deep Learning

### 1.2.Physics-Informed Learning

### 1.3.Information Bottleneck Method

### 1.4.Main Contributions

## 2.Analysis
### 2.1.Gradient Signal and Noise for SGD

### 2.2.Adam and Step-Wise SNR

### 2.3.Training Dynamics and Parameter Sensitivity

### 2.4.Residual Homogeneity

### 2.5.Adaptive Sample-Wise Weighting

## 3.Results
### 3.1.Total Diffusion and Convergence

### 3.2.Correlation of Adam and batch SNR

### 3.3.Information Compression

## 4.Summary
> In this study, we empirically demonstrated the existence of phase transitions proposed by the IB theory (fitting/diffusion) for PINNs trained with the Adam optimizer.
> Additionally, we identified a previously undisclosed third phase, which we named “total diffusion,” characterized by batch gradient agreement and fast convergence.
> Our findings highlight a strong correlation between this phase and gradient homogeneity, indicating that PINNs achieve optimal learning when residuals are diffused uniformly, leading to increased stability of the optimizer learning rate correction.
> To further support this hypothesis, we introduce a reweighting technique (RBA) to improve the vanilla PINN performance, which induces homogeneous residuals faster and improves generalization.

本项研究中, 我们经验性地证明了信息瓶颈理论 (拟合/扩散) 提出的相变对于使用 Adam 优化器训练的 PINNs 的存在性.
此外, 我们还识别出一个之前未发现的第三相, 称为 "完全扩散", 其特征是批量梯度一致和快速收敛.
我们的发现表明, 这一相合梯度同质性存在很强的相关性, 表明当残差均匀扩散时, PINNs 能够实现最佳学习, 使得优化器学习率校正的稳定性增加.
为了进一步支持这一假设, 我们引入了重加权技术以提升原始 PINN 的性能, 该技术可以更快地诱导出均匀的残差并提高泛化能力.

> Moreover, we investigate the relationship between signal-to-noise ratio (SNR) phase transition and information compression, observing a compression phenomenon due to the saturation of activations.
> As models transition abruptly into “total diffusion”, the network parameters rapidly increase, resulting in the saturation of neuron activations.
> Employing a relative-binning strategy, we find that while there is negligible information loss in deeper layers, there exists a hierarchy in the information content across layers, aligning with the principles of the IB theory.
> Our analysis reveals that middle layers exhibit the most pronounced saturation during the “binarization” process, reminiscent of an encoder-decoder mechanism within the fully-connected architecture.
> We believe this novel connection between IB theory and PINNs could open new possibilities for quantifying the performance of the many versions of PINNs and even neural operators.

此外, 我们还研究了信噪比相变与信息压缩之间的关系, 观察到由于激活函数的饱和导致的信息压缩现象.
当模型突然进入 "完全扩散" 时, 网络参数迅速增加, 从而导致神经元激活函数饱和.
采用相对分箱策略, 我们发现虽然更深层的信息损失可以忽略不记, 但各层之间的信息内容存在层次结构, 这与信息瓶颈理论相符.
我们的分析表明中间层在 "二值化" 过程中表现出最明显的饱和, 类似于全连接架构中的编码器-解码器机制.
我们认为信息瓶颈理论与 PINNs 之间的这种新联系可能为量化 PINNs 的各种版本乃至神经算子的性能开辟新的可能性.